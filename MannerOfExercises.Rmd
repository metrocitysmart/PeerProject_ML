---
title: "Predicting the manner of exercises - Peer Graded Assignment ML"
author: "G Raman Kumar"
date: "7th-Feb-2024"
output: html_document
---

```{r, setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE)
```

## Introduction
The goal of this assignment is to use data from accelerometers on the belt,
forearm, arm, and dumbell of 6 participants. They were asked to perform barbell
lifts correctly and incorrectly in 5 different ways. And predict the manner in
which they did the exercise. Train and test datasets provided for download
from the links mentioned on the Coursera project page, are mentioned below.

```{r, downloading_train_test}
trainUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
trainFilename <- "pml-traing.csv"
testFilename <- "pml-testing.csv"
# downloading and reading the train and test files
download.file(trainUrl, trainFilename)
download.file(testUrl, testFilename)
```

After downloading train and test files, read the files and loaded the datasets
into the workspace. Training data set has 19622 observations and 160 variables,
while the testing dataset has 20 observations and 160 variables. The training and testing data sets have a lot of NA values, empty strings and "#DIV/0!" character string as column values. Hence to treat all these as NAs re-read the file by providing **na.strings** values.

```{r, reading_datasets}
training <- read.csv(trainFilename, na.strings = c("NA", "", "#DIV/0!"))
testing <- read.csv(testFilename, na.strings = c("NA", "", "#DIV/0!"))
dim(training)
dim(testing)
```

## Pre-Process  

To determine which columns had NA values so that those columns
could be dropped, I obtained the column indexes for these columns. I dropped those columns. Further the first seven columns user names, column **X**, date and time and window id make no impact on class prediction hence, these columns also were dropped. All those columns that I dropped from training set I dropped them from **testing** set also to keep the training and testing sets similar.

```{r, noVariation_NAs}
# columns indexes with NAs and dropping those columns
col_inds <- which(colSums(is.na(training))==0)

workingTrain <- training[,col_inds]
# The first seven columns also have no impact on 
workingTrain <- workingTrain[,-c(1:7)]

# In order to keep the test set similar to the training set, dropped all those
# columns which were dropped from the training set.  
workingTest <- testing[ , col_inds]
workingTest <- workingTest[, -c(1:7)]
```

## Model training  
From the data sets obtained after pre-processing above, I created **train** and **validate** sets from *training* set, using **createDataPartition** from **caret** package. I choose random forest (**rf**) and **rpart** from caret package for training models for classification. For random forest I used 5-fold cross validation and obtained an overall accuracy of *0.995*. For **rpart** model used 10-fold cross validation and obtained a overall accuracy of **0.5**.
Out of sample errors for random forest and rpart models obtained by subtracting from 1 the overall accuracy and then multiplying the resulting number with 100, are *0.5%* and *50.0%* respectively. Hence used the random forest model for 20-sample test prediction. Below is the code for the entire model training section.

```{r, model_training}
library(caret)

set.seed(123)
intrain <- createDataPartition(y=workingTrain$classe, p = 0.75, list = FALSE)
train <- workingTrain[intrain,]
validate <- workingTrain[-intrain,]
train$classe <- factor(train$classe)
validate$classe <- factor(validate$classe)

# rf model and trainControl
ctrl <- trainControl(method = "cv", number = 5)
set.seed(235)
model1 <- train(classe ~ ., method = "rf", data = train, trControl = ctrl)
pred1 <- predict(model1, validate)
confusionMatrix(pred1, validate$classe)

# rpart model
ctrl_rpart <- trainControl(method = "cv", number = 10)
set.seed(385)
model2 <- train(classe ~ ., method = "rpart", data = train, trControl = ctrl_rpart)
pred2 <- predict(model2, validate)
confusionMatrix(pred2, validate$classe)

# Testing errors
oose <- (1 - confusionMatrix(pred1, validate$classe)$overall[1]) * 100
oose_rpart <- (1 - confusionMatrix(pred2, validate$classe)$overall[1]) * 100

# Prediction of the 20 different test cases using the rf model
predT1 <- predict(model1, workingTest)
predT1
```

## conclusion  
Used **random forest (rf)** from caret package to train the model, used 5-fold cross validation, obtained 0.5% out of sample error and predicted the 20-sample test from this model.